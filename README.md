# machine-learning-2-week-8-km3-solved
**TO GET THIS SOLUTION VISIT:** [Machine Learning 2 Week 8-KM3 Solved](https://www.ankitcodinghub.com/product/machine-learning-2-week-8-km3-solved/)


---

ğŸ“© **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
ğŸ“± **WhatsApp:** +1 419 877 7882  
ğŸ“„ **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;98840&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;0&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;0&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;0\/5 - (0 votes)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;Machine Learning 2 Week 8-KM3 Solved&quot;,&quot;width&quot;:&quot;0&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 0px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            <span class="kksr-muted">Rate this product</span>
    </div>
    </div>
<div class="page" title="Page 1">
<div class="layoutArea">
<div class="column"></div>
<div class="column">
&nbsp;

Exercise Sheet 8

</div>
</div>
<div class="layoutArea">
<div class="column">
Exercise 1: One-Class SVM (5+5+20+10+10 P)

The one-class SVM is given by the minimization problem:

</div>
</div>
<div class="layoutArea">
<div class="column">
12 1ô°„N min âˆ¥wâˆ¥ âˆ’Ï+ Î¾i

</div>
</div>
<div class="layoutArea">
<div class="column">
w,Ï,Î¾ 2 NÎ½i=1

</div>
</div>
<div class="layoutArea">
<div class="column">
s.t. âˆ€Ni=1 :âŸ¨Ï†(xi),wâŸ©â‰¥Ïâˆ’Î¾i and Î¾i â‰¥0

where x1, . . . , xn are the training data and Ï†(xi) âˆˆ Rd is a feature space representation.

<ol>
<li>(a) &nbsp;Show that strong duality holds (i.e. verify the Slaterâ€™s conditions).</li>
<li>(b) &nbsp;Write the Lagrange function associated to this optimization problem.</li>
<li>(c) &nbsp;Show the dual program for the one-class SVM is given by:</li>
</ol>
</div>
</div>
<div class="layoutArea">
<div class="column">
1NN

max âˆ’ ô°„ô°„Î±iÎ±jk(xi,xj)

</div>
</div>
<div class="layoutArea">
<div class="column">
Î± 2 i=1 j=1

ô°„N 1

</div>
</div>
<div class="layoutArea">
<div class="column">
s.t.

(d) Show that the problem can be equivalently rewritten in canonical matrix form as:

</div>
</div>
<div class="layoutArea">
<div class="column">
i=1

</div>
</div>
<div class="layoutArea">
<div class="column">
Î±i=1 and âˆ€Ni=1: 0â‰¤Î±iâ‰¤NÎ½ min 1Î±âŠ¤KÎ±

</div>
</div>
<div class="layoutArea">
<div class="column">
Î±2

âŠ¤ ô°‰âˆ’Iô°Š ô°‰0ô°Š

</div>
</div>
<div class="layoutArea">
<div class="column">
s.t. 1 Î±=1 and I Î±â‰¼ 1/NÎ½

where K is the Gram matrix whose elements are defined as Kij = k(xi,xj). (e) The decision rule in the primal for classifying a point as an outlier is given by:

âŸ¨Ï†(x), wâŸ© &lt; Ï

Also, one can verify that for any data point xi whose associated dual variable satisfies the strict inequalities

0 &lt; Î±i &lt; 1 , and calling one such point a support vector xSV, the following equality holds: NÎ½

âŸ¨Ï†(xSV), wâŸ© = Ï Show that the outlier detection rule can be expressed as:

NN

ô°„ Î±ik(x, xi) &lt; ô°„ Î±ik(xSV, xi)

i=1 i=1

Exercise 2: Programming (50 P)

Download the programming files on ISIS and follow the instructions.

</div>
</div>
</div>
<div class="page" title="Page 2">
<div class="section">
<div class="layoutArea">
<div class="column">
Exercise sheet 8 (programming) [SoSe 2021] Machine Learning 2

</div>
</div>
<div class="layoutArea">
<div class="column">
Implementing Anomaly Detection Models

In this exercise sheet, several kernel-based anomaly detection models will be implemented and their behavior compared on a simple two-dimensional dataset. The following code builds a dataset generated as a mixture of several Gaussian blobs.

</div>
</div>
<div class="layoutArea">
<div class="column">
In [1]:

</div>
<div class="column">
<pre>import sklearn.datasets
import sklearn.metrics
import numpy
import matplotlib
</pre>
from matplotlib import pyplot as plt %matplotlib inline

import utils

<pre>X = sklearn.datasets.make_blobs(n_samples=200,centers=10,random_state=2)[0]
X = X - X.mean(axis=0)
X = X / X.std() * 4.0
</pre>
utils.plot(X,None)

</div>
</div>
<div class="layoutArea">
<div class="column">
Kernel Density Estimation (10 P)

The first anomaly detection model is based on kernel density estimation (KDE). KDE builds the function

1N

f(x)= N âˆ‘k(x,xn)

n=1

where the output forms here an unnormalized probability density function. Note that if only interested in producing an ordering of points from least to most outlier, we donâ€™t need to normalize f(x). However, because f(x) is more a measure of inlierness than outlierness, we can define the outlier score o(x) as a decreasing function of f(x) and also make sure the function goes to infinity for very remote data points. This can be achieved with the scoring function:

o(x) = âˆ’ log(f(x))

We now would like to implement KDE using an interface similar to how ML algorithms are provided in scikit-learn, in particular, by defining a class that implements a fit function for training based on some training data X and a predict function for computing the prediction for a new set of points X . The KDE class is initialized with a kernel function (typically a Gaussian kernel). Its functions for training and predicting are incomplete.

Task:

Implement the functions fit(self,X) and predict(self,X) of the class KDE .

</div>
</div>
</div>
</div>
<div class="page" title="Page 3">
<div class="layoutArea">
<div class="column">
In [2]: class KDE:

def __init__(self,kernel):

self.kernel = kernel def fit(self,X):

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ # TODO: replace by your code

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ import solution solution.kde_fit(self,X)

<pre>                      # -----------------------------------
</pre>
return self

def predict(self,X):

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ # TODO: replace by your code

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ import solution

<pre>                      o = solution.kde_predict(self,X)
</pre>
<pre>                      # -----------------------------------
</pre>
return o

The KDE model can now be tested on our two-dimensional data. The code below passes to the KDE model a Gaussian kernel of scale Î³ = 0.25 (i.e. the bandwidth is slightly larger than for the default Gaussian kernel), train the model on the Gaussian blobs data, and apply the model to a grid dataset for the purpose of building a contour plot.

In [3]: kernel = lambda x,y: sklearn.metrics.pairwise.rbf_kernel(x,y,gamma=0.25) utils.plot(X,KDE(kernel).fit(X).predict(utils.Xgrid))

</div>
</div>
<div class="layoutArea">
<div class="column">
We observe that model behaves as expected, i.e. the regions outside the data are highlighted in red, which corresponds to high outlier scores.

</div>
</div>
</div>
<div class="page" title="Page 4">
<div class="section">
<div class="layoutArea">
<div class="column">
Uncentered Kernel PCA Anomaly Detection (15 P)

Another model for anomaly detection is based on Kernel PCA. Here, we consider an uncentered version of Kernel PCA where we do not subtract the mean of the data in feature space. Because it is not possible to compute exactly the eigenvectors from finite data, we resort to an empirical approximation based on the Gram matrix:

[K]nnâ€² = k(xn , xnâ€² ) and diagonalizing it to get empirical eigenvectors and eigenvalues:

K = UÎ›UâŠ¤

The matrix Î› is diagonal and contains all eigenvalues Î», â€¦ , Î»N sorted in descending order. The columns of the matrix U are the corresponding eigenvectors. For the training data, projection of the nth data point on the ith principal component is readily given by

proj(x )=U â‹…Î»0.5 in n,ii

</div>
</div>
<div class="layoutArea">
<div class="column">
For new data points x âˆˆ Rd , such projection is not readily available and we can resort instead to the following interpolation scheme: proj(x)=k(x,X)â‹…U â‹…Î»âˆ’0.5

</div>
</div>
<div class="layoutArea">
<div class="column">
i :,i i

The latter produces equivalent results for points (xn )n in the dataset but it generalizes the projection to any other point x âˆˆ Rd .

</div>
</div>
<div class="layoutArea">
<div class="column">
Once the data has been projected on the principal components, the outlier score can be computed as:

a

i=1

An incomplete version of uncentered kernel PCA anomaly detection is given below. Like for KDE, it receives a kernel as input, but one

also needs to needs to specify the number of dimensions used in the Kernel PCA model.

Task:

Implement the functions fit(self,X) and predict(self,X) of the class UKPCA .

In [4]: class UKPCA:

def __init__(self,kernel,dims): self.kernel = kernel self.dims = dims

def fit(self,X):

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ # TODO: replace by your code

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ import solution solution.ukpca_fit(self,X)

<pre>                      # -----------------------------------
</pre>
return self

def predict(self,X):

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ # TODO: replace by your code

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ import solution

<pre>                      o = solution.ukpca_predict(self,X)
</pre>
<pre>                      # -----------------------------------
</pre>
return o

The kernel PCA approach can now be tested. We first consider a kPCA model with a linear kernel and where we retain only the first principal component.

</div>
</div>
<div class="layoutArea">
<div class="column">
o(x) = k(x, x) âˆ’ âˆ‘ (proj (x))2 i

</div>
</div>
</div>
</div>
<div class="page" title="Page 5">
<div class="layoutArea">
<div class="column">
In [5]: kernel = sklearn.metrics.pairwise.linear_kernel utils.plot(X,UKPCA(kernel,1).fit(X).predict(utils.Xgrid))

</div>
</div>
<div class="layoutArea">
<div class="column">
The outlier score grows along the second principal component (the one with least variance). We now consider instead a Gaussian kernel (of slightly larger bandwidth than the one used for KDE) and build a the outlier function from a KPCA model containing 25 principal components.

In [6]: kernel = lambda x,y: sklearn.metrics.pairwise.rbf_kernel(x,y,gamma=0.1) utils.plot(X,UKPCA(kernel,25).fit(X).predict(utils.Xgrid))

</div>
</div>
<div class="layoutArea">
<div class="column">
Here, we observe that the outlier model much more closely follows the shape of the data distribution. However, we also observe that it saturates away from the data, which does not reflect the true degree of outlierness.

</div>
</div>
</div>
<div class="page" title="Page 6">
<div class="section">
<div class="section">
<div class="layoutArea">
<div class="column">
One-Class SVM (25 P)

The one-class SVM is another approach to anomaly detection that aims to build some envelope that contains the inliner data and that separates it from outlier data. In its dual form, it consists of solving the constrained optimization problem:

</div>
</div>
<div class="layoutArea">
<div class="column">
subject to

</div>
<div class="column">
min 1Î±âŠ¤KÎ± Î±2

1âŠ¤Î±=1 and (âˆ’I)Î±âª¯( 0 ) I 1/NÎ½

</div>
</div>
<div class="layoutArea">
<div class="column">
To solve this optimization problem, we can use the quadratic solver provided as part of cvxopt and the interface of which is shown below:

Once the solution has been found, the output score can be computed as f(x) = âˆ‘i Î±ik(x,xi). Similarly to the outlier scores we have computed for KDE, we can build a transformation

o(x) = âˆ’log âˆ‘i Î±ik(x,xi) âˆ‘i Î±ik(xSSV,xi)

where xSSV is any â€˜strictâ€™ support vector (they can be identified as implementing the box constraints above with strict inequalities). With this transformation the equation o(x) = 0 also gives the OC-SVM decision boundary.

Task:

Implement the functions fit(self,X) and predict(self,X) of the class OCSVM .

In [7]: import cvxopt

import cvxopt.solvers

class OCSVM:

def __init__(self,kernel,nu): self.kernel = kernel self.nu = nu

def fit(self,X):

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ # TODO: replace by your code

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ import solution solution.ocsvm_fit(self,X)

<pre>                      # -----------------------------------
</pre>
return self

def predict(self,X):

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ # TODO: replace by your code

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ import solution

<pre>                      o = solution.ocsvm_predict(self,X)
</pre>
<pre>                      # -----------------------------------
</pre>
return o

The OC-SVM can now be tested on the 2d dataset. Here, we first consider the case where Î½ = 0.0001, which corresponds to implementing a hard envelope (with no points outside of it).

</div>
</div>
</div>
</div>
</div>
<div class="page" title="Page 7">
<div class="layoutArea">
<div class="column">
In [8]:

</div>
<div class="column">
kernel = lambda x,y: sklearn.metrics.pairwise.rbf_kernel(x,y,gamma=0.1) utils.plot(X,OCSVM(kernel,0.0001).fit(X).predict(utils.Xgrid),boundary=True)

<pre>     pcost       dcost       gap    pres   dres
 0:  5.9756e-02 -1.0097e+04  1e+04  1e-13  2e-13
 1:  5.9751e-02 -1.0853e+02  1e+02  3e-15  4e-13
 2:  5.9521e-02 -4.9829e+00  5e+00  2e-16  2e-14
 3:  7.0927e-02 -4.2701e+00  4e+00  4e-16  1e-14
 4:  7.4758e-02 -3.5860e+00  4e+00  4e-16  2e-14
 5:  6.8449e-02 -1.8468e-01  3e-01  2e-16  2e-15
 6:  6.2819e-02 -1.0865e-01  2e-01  2e-16  9e-16
 7:  5.9641e-02  1.9806e-02  4e-02  3e-16  8e-16
 8:  5.5603e-02  3.9579e-02  2e-02  7e-16  7e-16
 9:  5.4303e-02  4.7196e-02  7e-03  6e-16  7e-16
</pre>
<pre>10:  5.3536e-02  5.1293e-02  2e-03  2e-16  6e-16
11:  5.3182e-02  5.2474e-02  7e-04  4e-16  7e-16
12:  5.3067e-02  5.2747e-02  3e-04  4e-16  7e-16
13:  5.2983e-02  5.2925e-02  6e-05  4e-16  6e-16
14:  5.2968e-02  5.2951e-02  2e-05  4e-16  6e-16
15:  5.2963e-02  5.2960e-02  3e-06  2e-16  7e-16
16:  5.2962e-02  5.2961e-02  3e-07  2e-16  7e-16
17:  5.2961e-02  5.2961e-02  7e-09  2e-16  7e-16
Optimal solution found.
</pre>
</div>
</div>
<div class="layoutArea">
<div class="column">
We observe that all points are indeed either contained in the envelope or at the border of it. We can now test the OC-SVM with a larger parameter Î½, here, Î½ = 0.1 and run the code again:

</div>
</div>
</div>
<div class="page" title="Page 8">
<div class="layoutArea">
<div class="column">
In [9]:

</div>
<div class="column">
kernel = lambda x,y: sklearn.metrics.pairwise.rbf_kernel(x,y,gamma=0.1) utils.plot(X,OCSVM(kernel,0.5).fit(X).predict(utils.Xgrid),boundary=True)

<pre>     pcost       dcost       gap    pres   dres
 0:  5.9756e-02 -1.9792e+00  4e+02  2e+01  2e-15
 1:  6.6548e-02 -1.9549e+00  6e+00  2e-01  4e-15
 2:  7.2113e-02 -8.7545e-01  9e-01  4e-16  3e-15
 3:  7.0349e-02  1.9189e-02  5e-02  3e-17  1e-15
 4:  6.5181e-02  5.4344e-02  1e-02  6e-16  9e-16
 5:  6.3039e-02  5.9564e-02  3e-03  9e-17  7e-16
 6:  6.2249e-02  6.0824e-02  1e-03  2e-17  6e-16
 7:  6.1873e-02  6.1403e-02  5e-04  1e-16  6e-16
 8:  6.1728e-02  6.1573e-02  2e-04  3e-16  7e-16
 9:  6.1671e-02  6.1646e-02  3e-05  1e-16  8e-16
</pre>
<pre>10:  6.1661e-02  6.1659e-02  2e-06  5e-16  7e-16
11:  6.1660e-02  6.1660e-02  4e-08  4e-16  7e-16
Optimal solution found.
</pre>
</div>
</div>
<div class="layoutArea">
<div class="column">
This time, not all data points are contained in the envelope, and some of them are therefore classified by the model as outlier.

</div>
</div>
</div>
<div class="page" title="Page 9"></div>
<div class="page" title="Page 10"></div>
